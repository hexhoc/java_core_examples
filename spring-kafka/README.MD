# SPRING BOOT KAFKA EXAMPLE

## Описание
This example contains 3 microservices:
1. **producer-1**. For send message in topics (**topic_1_request, topic_2_request, topic_all_request**) and listen topics (**topic_1_response, topic_2_response, topic_all_response**)
2. **consumer-1**. Listen message from topics (**topic_1_request, topic_all_request**) and send response (**topic_1_response, topic_all_response**) 
2. **consumer-2**. Listen message from topics (**topic_2_request, topic_all_request**) and send response (**topic_2_response, topic_all_response**) 

## Установка и запуск
1. Запустить `cd docker-compose/kafka-example` и затем `docker-compose up -d -b`
2. Открыть http://localhost:9000 и проверить kafdrop
3. Запустить продюсер. Продюсер создает 3 топика - **topic_1_request, topic_2_request, topic_all_request**
4. Отправить простое сообщение **GET** http://localhost:8080/produce/topic_1_request/Test message
5. Отправить пачку сообщений **GET** http://localhost:8080/produce/highload/topic_1_request?messageCount=1000&startValue=0&timeGap=100 
   
   Эта команда отправляет 1000 сообщений с задержкой 100 ms
6. Запускаем консьюмер 1
7. Запускаем консьюмер 2


## ТЕРМИНОЛОГИЯ

*Kafka cluster* - группа объединенных нод.

- *node, broker, server* - это все сама Кафка. Отвечает за прием,
  хранение и отправку.

- *topic/partition* - почтовый ящик в который складываем сообщения и
  забираем оттуда

- *zookeeper* - база данных метаданных, которая хранит информацию о серверах в кластере и их статусах, топиках, где какие топики, партиции и т.п.

- *producer* - тот кто передает сообщение

- *consumer* - тот кто принимает сообщение

- consumer group - consumers которые могут объединяться в группу для чтения данных из одного топика. Offset для consumer group общий. т.е. два разных consumer в одной группе не смогут прочитать одно и то же сообщение.

https://habr.com/ru/articles/530714/

https://www.springcloud.io/post/2022-10/springboot-kafka-tx/#gsc.tab=0
## TOPICS

Все сообщения поступают и считываются из topic.

Topic работает по fifo, после чтения из него, сообщения не удаляются.
Это нужно например на случай, если мы подключим еще одного consumer и он
считает все сообщения из очереди которые там есть

## CONSUMERS GROUP

Несколько consumers могут объединятся в группу чтобы потреблять сообщения из топика(-ов)

**==Использование групп является обязательным в kafka. Если группа фактически не указана, то будет создана рандомная группа==**

**If two consumers have subscribed to the same topic and are present in the same consumer group, then these two consumers would be assigned a different set of partitions and none of these two consumers would receive the same messages.**

Группы помогают горизонтально масштабировать consumers для высокой скорости считывания сообщений из топиков. Мы можем горизонтально масштабировать consumers в kubernetes и они все будут в рамках одной группы читать сообщения.

Case 1: If a new consumer joins the consumer group, rebalancing happens and each consumer is now assigned to a single partition (since we have equal number of partitions and consumers).

**Case 2**: If a consumer goes down, then there’d be only 1 consumer left in the consumer group and all the partitions would be assigned to this consumer through rebalancing.

**Rebalancing** is the re-assignment of partition ownership among consumers within a given consumer group such that every consumer in a consumer group is assigned one or more partitions.
## PARTITIONS

**Внимание! На 1 партицию, может быть назначен только 1 consumer в пределах конкретной consumers group. Когда на 1 партицию подписаны несколько consumers из одной группы, это теряет всякий смысл. Смысл партиций в том, чтобы распараллелить чтение данных для consumer. 1 партиция = 1 consumer, 2 партиции = 2 consumer**

По умолчанию топик создается с 1 партицией. У каждой партиции свой собственный offset

Когда у нас 1 consumer и 4 партиции, то он читает из всех.

Когда 2 consumer, то они делят партиции поровну

Когда 4 consumer, то каждый забирает на себя по 1 партиции.

Когда 5 consumers, то 4 читают, 1 простаивает.

Если мы хотим увеличить производительность чтения и записи данных из топика, то необходимо топик разбить на несколько партиций, а партиции в свою очередь распределяться на разные брокеры.

В рамках consumers group каждый consumer будет читать свое сообщение, у группы общий offset поэтому исключена ситуация когда два consumer в рамках одной группы прочитают одно и тоже сообщение. Если мы хотим чтобы одни и те же сообщения читались повторно, то необходимо создать отдельную consumers group и подписать ее на топики

Топик можно разбиваться на партиции,  каждая патриция это своя очередь в
рамках одного топика, разные консьюмеры могу потреблять сообщения из своей партиции. Порядок fifo работает только для партиции а не для
топика в целом. Если мы положили сообщение 1 в одну партицию, сообщение
2 в другую, не факт что они будут считаны в том же порядке.

каждая ведущая реплика партиции существует в одном брокере Kafka и не может разбиваться на более мелкие части

Producer определяет сообщение в конкретную партиции по ключу, который передает вместе с сообщением, он вычисляет хешкод ключа и исходя из него определяет партицию. Если ключа нет, то он рассчитывает хеш код автоматически

ПРОБЛЕМА 1. ПОРЯДОК СООБЩЕНИЙ НАРУШАЕТСЯ ПРИ УВЕЛИЧЕНИИ ПАРТИЦИЙ.
При создании нескольких партиций порядок сообщений становится нарушенным, мы можем решить эту проблему если будем группировать данные по ключу, в пределах одного ключа, данные будут последовательны. Например можем группировать все события конкретного заказа по номеру заказа. **Важно! По ключу Kafka рассчитывает hash и по хэшу определяет в какую партицию положить сообщение. Сообщения с одинаковым ключем всегда попадут в одну партицию**

ПРОБЛЕМА 2. РАСШИРЕНИЕ ПАРТИЦИЙ ПРИВОДИТ К ТОМУ ЧТО СООБЩЕНИЯ С КЛЮЧЕМ МОГУТ СМЕНИТЬ НОМЕР ПАРТИЦИИ.

Если у нас 2 партиции, и мы для группировки сообщений используем ключ, благодаря которому сообщения всегда падают только в конкретную партицию, то при увеличении партиций до 3 штук, сообщения с нашим ключем могут начать попадать в партицию с другим номером. Это становится проблемой если consumer ожидает получение нужных ему сообщений из конкретного номера партиции

ПРОБЛЕМА 3. УМЕНЬШЕНИЕ ПАРТИЦИЙ НЕ ВОЗМОЖНО.

Возможно, но это деструктивная операция приводящая к удалению данных. Т.к. требуется удалять партицию и выполнять перебалансировку

## ХРАНЕНИЕ

Все данные топика хранятся в обычных лог файлах. Есть папка logs в ней
папки с названием топика и партиции topicA-0 topicA-1. В папке топика 3
файла .log .index и .timeindex

![image info](./static/kafka/1.jpg)

В файле log хранятся все сообщения топика

Файлы не бесконечные и при достижении определённого лимита, создаются
новые файлы(сегмент) с тем же расширением. Старые архивируются и больше не используются.

Удалять сообщения **НЕЛЬЗЯ** никак. Но можно настроить автоудаление
сегментов по TTL.

В каждом сегменте (это группа файлов) есть данные timestamp. Мы можем
указать что есть последний timestamp в сегменте превысил определённый
срок, то удаляем ВЕСЬ сегмент. Частичного удаления нет.

## РЕПЛИКАЦИЯ

Для каждого топика обязательно установлен параметр replication factor, по умолчанию он равен 1. Каждая партиция имеет leader и 0 или более followers.

Реплики всегда отстают от лидер на определенную дельту потому что последовательность записи и чтения в лидера происходит всегда в первую очередь а дальше информация дублируется в реплики, поэтому всегда есть возможность потерять данные в случае если лидер реплики умрет. Чтобы не случилось подобной проблемы, нужно установить у producer параметр **==acks=all==** чтобы продюсер обязательно дождался когда отправленное сообщение будет записано в реплики. Затем нужно установить минимальное колво синхронизированных реплик параметром **==min.insync.replicas==** (если реплик 4, а нам достаточно иметь всего 2 в синхроне, то ставим 2)

Партиции топика реплицируются между нодами. Одна из реплик назначается
==лидером== и запись происходит **ТОЛЬКО** в нее. Остальные реплики назначаются
фолловерами. Фолловеры время от времени опрашивают лидера для загрузки
данных к себе.

Для избежания потери данных для каждой партиции мы устанавливаем
replication factor, который реплицирует данные партии на другие брокеры
в кластере.

## ПОТВЕРЖДЕНИЕ СООБЩЕНИЙ.


Продюсер отправляет сообщения в брокер. В самом сообщении можно указать признак acks, который имеет следующие значения:

**0** - продюсер не ждёт подтверждение о доставке сообщения. После
отправки сообщения оно по умолчанию считается доставленым.

**1** - продюсер ждет подтверждение о доставке сообщения в лидеры
реплики, ему не интересна запись в **in-sync replica (ISR)**. Может быть
проблема, что мы получили потверждение от лидера реплики, а потом сразу нода с лидером реплики упала. Тогда сообщение пропадает.

**All** - продюсер ждет подтверждения от всех **in-sync replica (ISR)**

### МОДЕЛЬ ДОСТАВКИ (DELIVERY MODEL)
Все приведенные ниже модели доставки не являются какими то значениями параметра, а являются семантикой, которую мы можем для своего проекта выбрать

1. **at-most-once**. Нас не волнует процессинг сообщения, сразу как только consumer получил сообщение, сообщение считает обработанным.
   **At-most-once** consumer is the default behavior of a KAFKA consumer.
   Чтобы включить его нужно:
   - на стороне producer установить `acks=0`. Чтобы не ждать подтверждения о доставке
   - на стороне consumer установить `enable.auto.commit = true`.

3. **at-least-once**. Сообщение считает доставленным после получения и последующего процессинга этого сообщения. Ручное управление коммитом.
   - *Достаточно на стороне consumer включить ручное управление commit* `enable.auto.commit = false`
   - или включить `enable.auto.commit = true` вместе с `auto.commit.interval.ms` установив высокое значение

4. **exactly-once**. В данном случае повторная обработка одного и того же сообщения не должна оказывать никакого сайд эффекта. Т.е. обработчик сообщения должен быть идемпонентным, либо на стороне получателя должен хранится признак, что сообщение уже было обработано. ВАЖНО! Так же можно использовать механизм транзакций Кафки (transactional API). Для включения в spring boot у producer задаем параметр `spring.kafka. producer.transaction-id-prefix='prod-1'` и поставить аннотацию `@Transactional`. На стороне consumer просто ставим аннотацию `@Transactional`
   *==Достигается путем включения ручного подтверждения commit на стороне consumer, и получение сообщения должно быть либо индепонентным или мы должны у себя в базе вести свой offset и проверять его при каждой загрузке сообщения==*.
   **Так же индепонентность должна быть включена на стороне продюсера, чтобы избежать повторной отправки сообщения в топик `idempotence=true` Потребитель сам должен позаботиться о том, чтобы не обработать одну и ту же запись несколько раз.**

## ОТПРАВКА СООБЩЕНИЙ.

### PRODUCE:

**ВАЖНЫЕ ПАРАМЕТРЫ:**
1. **acks**. Кол-во подтверждений которые producer требует для того чтобы понять что данные отправлены.
   **acks=0.** Producer не ждет никак з подтверждений. Сообщение заведомо считает отправленным. В данном случае параметр **retries** не имеет смысла
   **acks=1**. Получает подтверждение от лидера партиции о том что сообщение доставлено
   **acks=all.** Получает подтверждение от лидера партиции и всех реплик о доставке
   *Default: all*
1. **bootstrap.servers**. A list of host/port pairs to use for establishing the initial connection to the Kafka cluster
2. **key.serializer**
3. **value.serializer**
4. **retries**. Попытки отправки сообщений в броке, в том случае, если отправить из не получается. Попытки прекратятся если время отправки запроса будет превышать значение е в параметре delivery.timeout.ms.
   *Default: 2147483647*
6. **batch.size**. Продюсер пытается объединить сообщения в пакет, если несколько сообщений отправляются в одну партицию. Указывает размер пакета  в байтах. Указывать этот параметр нужно вместе с linger.ms (указывает как долго ждать перед отправкой, по умолчанию 0). Даже если параметр указан, а linger.ms мы оставили пустым, то сообщение будет отправлено не дожидая наполнения пакета.
   *Default: 16384 bytes*
7. **max.request.size**. Ограничивает размер отправляемого сообщения.
   *Default: 1048576 bytes*
9. **enable.idempotence**. Контролирует чтобы только одна копия сообщения была доставлена в брокер. Требует чтобы были включены параметры **max.in.flight.requests.per.connection** меньше 5, **retries** больше 0, **acks=all.**
   *Default: true*
10. **max.in.flight.requests.per.connection.** количество неподтверждённых запросов которые клиент может отправить на одно соединение параллельно. Т.е. у нас стоит acks=all и каждое сообщение должно ожидать подтверждение, пока оно ждет подтверждения, другие сообщения не отправляются. Если поставим этот параметр равным 5, то ожидание подтверждения станет параллельным и до 5 сообщений могут стоять одновременно в очереди за подтверждением.
    *Default: 5*


**ПОРЯДОК ОТПРАВКИ:**

1.  Producer выполняет операцию fetch metadata к брокеру, брокер
    к zookeeper. Он должен знать из чего состоит класстер, какие топики
    есть, на какие партиции они разделены, где и какие реплики, кто
    является лидером реплики (отправляет только лидеру реплики)

2.  Producer сериализует сообщение

3.  Выбор в какую партицию отправить сообщение. 3 варианта:
    **Exclicit-partition** - мы явно указываем партицию.
    **Round-robin** - циклично отправляем сообщение в каждую партицию.
    **Key-defined** - самый часто используемый алгоритм, он определяет
    партицию автоматически по ключу сообщения. Если ключ каждого
    последующего сообщения будет одинаковый, то гарантированно сообщение
    будет доставлено в одну и ту же партицию.

4.  Компрессия сообщения.

5.  Сообщения аккумулируются в batch перед отправкой. Batch при
    достижении определенного размера отправляется в брокер. Для
    определения batch используется параметр **batch size **

6.  Отправляем сообщение

 

### CONSUME:

Обычно чтение сообщение происходит пачками. Считываются сразу 3 сообщения, и подтверждение offset происходит по номеру последнего из них

**ВАЖНЫЕ ПАРАМЕТРЫ:**
1. **heartbeat.interval.ms**. Ожидаемое время между heartbeat для координатора по работе с consumers при использовании средств управления группами Kafka. Частота heartbeat используется для обеспечения того, чтобы сеанс пользователя оставался активным, и для облегчения восстановления баланса, когда новые пользователи присоединяются к группе или покидают ее. Значение должно быть установлено ниже, чем session.timeout.ms
   *Default: 3000 (3 seconds)*
2. **session.timeout.ms**. Тайм-аут, используемый для обнаружения сбоев клиента при использовании средства управления группами Kafka. Клиент периодически посылает брокеру сигналы о том, что он жив. Если брокер не получит никаких сообщений до истечения этого тайм-аута сеанса, то брокер удалит этого клиента из группы и инициирует перебалансировку.
   *Default: 45000 (45 seconds)*
3. **auto.offset.reset**. Что делать, если в Kafka нет начального offset или если текущее offset больше не существует на сервере (например, потому что эти данные были удалены):
- earliest: автоматически сбрасывает смещение на самое раннее смещение
- latest: автоматически сбрасывает смещение на самое последнее смещение
- nothing: выдает исключение потребителю, если предыдущее смещение не найдено для группы потребителя
- another else: выдает исключение потребителю.
  *Default: latest*
4. **enable.auto.commit**. Если значение true, смещение потребителя будет периодически фиксироваться в фоновом режиме. По умолчанию включен, поэтому нет гарантий что сообщение будет считано повторно при ошибке его обработки. Если хотим иметь гарантию что все сообщения будут обработаны, то ставим тут false и коммтим сообщения вручную.
   *Default: true*

**СКОЛЬКО СООБЩЕНИЙ СЧИТЫВАЕТ ЗА ОДИН POLL**

Kafka всегда считывает сообщения пакетами. Кол-во считываемых сообщений определяется параметром **max.poll.records** default:500. если за раз мы хотимм считывать только одно сообщение то необходимо установить 1


**МНОГОПОТОЧНОЕ ЧТЕНИЕ**

The @KafkaListener annotation in Spring Kafka runs in a single thread by default. This means that if you have multiple partitions in your Kafka topic, the messages will be consumed sequentially, one at a time. However, you can configure the concurrency of the listener using the concurrency property of the annotation to specify the number of threads to use for message consumption. For example, @KafkaListener(topics = "myTopic", concurrency = "3") would create three threads to consume messages from the "myTopic" topic.

**ПОРЯДОК СЧИТЫВАЕМЫХ СООБЩЕНИЙ**

Сообщения всегда считываются по порядку в том случае, если у нас топик с одной партицией.

Порядок чтения сообщений может отличаться если используются несколько партиций в рамках одного топика. Или когда в одной группе несколько consumers

Или когда попытка отправить сообщение 1  провалилось, сразу за ним отправилось сообщение 2, а потом пошла вторая попытка отправить сообщение 1. Т.е. порядок уже нарушен.


**Решения:**
1. использовать одну партицию и одного consumer.
2. Использовать какое-нибудь значение в качестве ключа партиции. Сообщения с одним ключем будут попадать в одну партицию. У нас есть сообщения сообщения ORDER_CREATE, ORDER_MODIFY, and ORDER_CANCEL, мы для них устанавливаем один ключ и все они попадут в одну партицию. Идеально к примеру использовать в качестве ключа Order ID.
3. Или в каждую партицию мы можем группироваться сообщения по алфавиту. Например мы отправляем сообщения с и ценами покупателей, в одну партицию записываем сообщения начинающиеся на букву A-M в другую партицию с N-Z.

**ПОВТОРНОЕ ЧТЕНИЕ (DUPLICATE MESSAGES)**


сообщение может быть прочитано несколько раз, по следующим причинам:
1. В тот момент когда мы считываем пачку сообщений из кафки, у нас есть интервал в пределах которого мы должны успеть обработать сообщения (max.pool.interval). Если за этот интервал сообщения не успелись отработать, то Кафка считает что consumer умер и выполняет rebalance партиций, и если у нас есть другой инстанс приложения в группе, то он передает считывание этой пачки сообщений туда.
2. Если используется стратегия **only once** (enable.auto.commit to true, это значит что kafka ставит offset сразу как отправил сообщение в consumer) Consumer считал сообщение из Kafka но аварийно завершился, не успев его обработать и записать в БД. В таком случае в сообщение в брокере может быть о семейной как считанное, но по факту оно не было обработано.
3. Если enable.auto.commit to false, это значит что kafka ставит offset только тогда когда мы отработаем все сообщения и подтвердим что они обработаны. в таком случае если мы считали пачку сообщений (ORDER_CREATE, ORDER_MODIFY, and ORDER_CANCEL) успешно обработали ORDER_CREATE, и не смогли обработать остальные, поэтому подтверждение о получении не отправили. В таком случае кайуа по новой отправит эту пачку сообщений.



Чтобы избежать подобных случаев, но нужно предпринять следующие шаги:
1. Убедиться в настройках producer чтобы исключить повторную отправку сообщений
2. По возможности сделать consumer handler индепонентным. Чтобы появление дублей сообщений возвращало один и тот же результат
3. Consumer записывает в БД id принятых сообщений. Если они уже были приняты то повторно не обработаться.

В том случае если наш consumer реплицируется в kubernetes можно не беспокоится о том, что он. в один момент времени могут прочитать одно и тоже сообщение. group-id гарантирует что одно сообщение будет прочитано только одним consumer входящим в группу.


**ПОРЯДОК  ЧТЕНИЯ**

1.  **Consumer** выполняет операцию **fetch metadata** к брокеру, брокер
    к **zookeeper**. Он должен знать из чего состоит класстер, какие
    топики есть, на какие партиции они разделены, где и какие реплики,
    кто является лидером реплики (принимае только из лидера реплики)

2.  Устанавливаем соединение с брокерами в которых находятся
    нужные топики. Если **consumer** хочет читать сразу весь топик, то
    он подключается сразу ко всем leader партициям на всех **нодах**

3.  Считывает значение значение **offset** из служебного топика
    **\_\_consumer\_offset** чтобы исключить чтение уже
    прочитанных сообщений.

4.  Если **consumer** будет обрабатывать сообщения в один поток, то это
    будет медленно, мы можем создать группу **Consumer group** в которой
    сам consumer бьется на несколько частей, каждая из которых считывает
    данные из **kafka** в отдельном потоке

5.  В **kafka** есть отдельный топик **\_\_consumer\_offset**. Каждый
    раз когда **consumer** считывает сообщения из топика, после этого он
    отправляет offset в этот служебный топик, чтобы эти сообщения
    повторно не читались.

 
## МОЖЕТ ЛИ ПРИЛОЖЕНИЕ ЧИТАТЬ И ОБРАБАТЫВАТЬ СООБЩЕНИЯ ПАРАЛЛЕЛЬНО
 

Когда вы аннотируете метод аннотацией @KafkaListener, он создает **message listener container**, который может принимать сообщения из одной или нескольких топиков Kafka. По умолчанию этот контейнер создаст один или несколько экземпляров Kafka consumer, в зависимости от настроенных вами параметров параллелизма, которые могут параллельно считывать сообщения из топиков. Это означает, что если в тематических разделах, используемых контейнером, доступно несколько сообщений, consumers будут читать и обрабатывать их одновременно, вплоть до максимального уровня concurrency, который вы установили.

## ЧТО ТАКОЕ message listener container?

message listener container - это компонент Spring Kafka, который обеспечивает более высокий уровень абстракции по сравнению с Kafka consumer API. Он обрабатывает создание и управление жизненным циклом одного или нескольких экземпляров Kafka consumer, а также обработку сообщений, полученных этими **consumers**.

Когда вы аннотируете метод аннотацией **@KafkaListener**, Spring Kafka создает контейнер прослушивателя сообщений, который настроен для приема сообщений из указанной . Контейнер отвечает за создание одного или нескольких экземпляров Kafka consumer на основе настроенных вами параметров параллелизма и за распределение сообщений, полученных этими consumers, по соответствующим методам прослушивания.

## СООБЩЕНИЯ ЧИТАЮТСЯ ПОСЛЕДОВАТЕЛЬНО ИЛИ В РАЗНОМ ПОРЯДКЕ?

Сообщения Kafka считываются последовательно внутри partition. Однако сообщения из нескольких разделов могут обрабатываться параллельно, что означает, что порядок сообщений в разных partitions может не сохраняться. Таким образом, внутри partition порядок сообщений сохраняется, но в разных partitions порядок может варьироваться в зависимости от реализации consumer.

## В КАКИХ СЛУЧАЯХ СООБЩЕНИЕ В КАФКЕ МОЖЕТ БЫТЬ ЗАПИСАНО В ТОПИК ДВАЖДЫ

Если мы установили параметр acks=all то продюсер будет ждать уведомление о доставке от всех брокеров. Но если он вдруг не дождался (какой-то брокер отвалился) то продюсер отправит сообщение повторно, и тут может произойти дублирование.

Для решения проблемы необходимо на стороне продюсера включить `idempotence=true` что исключает повторное попадание сообщения в топик.

## В КАКИХ СЛУЧАЯХ СООБЩЕНИЕ В КАФКЕ МОЖЕТ БЫТЬ ПРОЧИТАННО ДВАЖДЫ

Есть несколько случаев, когда сообщение может быть прочитано из topic Kafka несколько раз при использовании аннотации @KafkaListener в Spring Boot:

Consumer group rebalancing: Когда новый consumer присоединяется к группе или существующий consumer покидает группу, координатор группы запускает перебалансировку. В течение этого времени partiion переназначаются consumer в группе, что может привести к повторному считыванию сообщений consumer с его предыдущего offset.

сообщение может быть прочитано несколько раз, по следующим причинам:
1. В тот момент когда мы считываем пачку сообщений из кафки, у нас есть интервал в пределах которого мы должны успеть обработать сообщения (max.pool.interval). Если за этот интервал сообщения не успелись отработать, то Кафка считает что consumer умер и выполняет rebalance партиций, и если у нас есть другой инстанс приложения в группе, то он передает считывание этой пачки сообщений туда.
2. Если используется стратегия **only once** (enable.auto.commit to true, это значит что kafka ставит offset сразу как отправил сообщение в consumer) Consumer считал сообщение из Kafka но аварийно завершился, не успев его обработать и записать в БД. В таком случае в сообщение в брокере может быть о семейной как считанное, но по факту оно не было обработано.
3. Если enable.auto.commit to false, это значит что kafka ставит offset только тогда когда мы отработаем все сообщения и подтвердим что они обработаны. в таком случае если мы считали пачку сообщений (ORDER_CREATE, ORDER_MODIFY, and ORDER_CANCEL) успешно обработали ORDER_CREATE, и не смогли обработать остальные, поэтому подтверждение о получении не отправили. В таком случае кайуа по новой отправит эту пачку сообщений


## ТИПЫ КОММИТОВ

В качестве подтверждения о получении сообщения, consumer отправляет
коммит подтверждающий это

**Коммиты бывают:**

1.  **Auto commit**. Сразу после получения сообщения, отправляет
    уведомление о получении. **ПРОБЛЕМА**: Однако если по какой-то
    причине нам не удалось обработать это сообщение, повторно уже мы его
    получить не сможем, потому что потвердили его получение.

2.  **Manual commit**. Вручную отправляем подтверждение о получении.
    Например мы приняли сообщение, обработали, записали в БД, и уже
    после этого отправляем потверждение. **ПРОБЛЕМА**: сообщения всегда
    читаются пачками, допустим всего взяли 3 сообщения, 2 обработали, а
    на последнем упали. В итоге не потвердили, а значит будет еще раз
    все 3 сообщения загружать повторно.

3.  **Custom offset**. Мы сами пишем свой механизм регистрации offset.


## СБРОС OFFSET

В ряде случаев когда мы пропустили какое-то важное сообщение из-за ошибки и хотим прочитать еще раз, мы можем выполнить сброс offset назад до нужного значения, или же когда в топик загрузилось ошибочное сообщение, которое невозможно обработать, мы можем сдвинуть offset вперед чтобы пропустить сообщение

`/bin/kafka-consumer-groups.sh \ --bootstrap-server localhost:9092 \ --topic getting-started --group cli-consumer \ --reset-offsets --to-earliest --execute` сдвигаем offset в начало топика

`/bin/kafka-consumer-groups.sh \ --bootstrap-server localhost:9092 \ --topic getting-started:0,1 --group cli-consumer \ --reset-offsets --to-offset 2 --execute` сдвигаем offset для партиций 0 и 1 на offset = 2

`/bin/kafka-consumer-groups.sh \ --bootstrap-server localhost:9092 \ --topic getting-started:2 --group cli-consumer \ --reset-offsets --to-datetime 2020-01-27T14:35:54.528+11:00 \ --execute` сброс offset на конкретное время. Обратите внимание что важно учитывать timezones

Так же обратить внимание на параметр `--shift-by` который позволяет сдвинуть offset на указанное колво шагов относительно текущего offset например если укажем 1, то сдвинем текущий offset на 1. Если указать -1 то сдвинем на шаг назад

`/bin/kafka-consumer-groups.sh \ --bootstrap-server localhost:9092 \ --topic getting-started --group cli-consumer --delete-offsets` полное удаление offset. Это значит что установка нового значения offset возлагается на consumer и зависит от его параметра auto.offset.reset (to earliest или to latest)

`/bin/kafka-consumer-groups.sh \ --bootstrap-server localhost:9092 \ --group cli-consumer --delete` удаление consumer group эквивалентно ужалению offset


## УДАЛЕНИЕ СООБЩЕНИЙ

Удалять сообщения в кафке невозможно. Можно либо удалить топик целиком, либо установить срок хранения сообщений после которого они будут удаляться.

Мы можем только пропустить конкретное сообщение сдвинув offset

## ПОЧЕМУ KAFKA СЧИТАЕТСЯ БЫСТРОЙ

1.  **Расширяемая архитектура**. Возможность создать кластер и добавлять
    туда ноды, в ноды партиции и реплики

2.  **Последовательное чтение и запись**. Данные при последовательном
    доступе индексируют и обрабатываются намного быстрее

3.  **Zero copy**. Сообщение ровно в том виде в котором оно отправляется
    получателю, ровно в таком виде и получает,
    никаких доп.трансформаций. На уровне операционной системы подобная
    передача данных оптимизирована.

4.  **Очень гибкие настройки**. Под каждый отдельный случай кафку можно
    настроить, за счет этого ее работа будет всегда оптимальной.

## УПРАЖНЕНИЯ

### EASY

1. producer-1 передает сообщение в топик topic_all_request. consumer-1 считывает его и возвращает ответ. producer-1 считывает ответ
2. Подключаем consumer-2 к топику topic_all_request. он должен начать считывать сообщения с того же offset что и consumer-2.
3. producer-1 передает сообщение в общий топик, его считывают consumer-1 и consumer-2 и возвращают ответ. producer-1 считывает ответ
4. producer-1 помещает сообщения в topic_all_request но в разные партиции. consumer-1 и consumer-2 считывают из топика не в зависимости от партиции
5. Каждый consumer считывает сообщения из своей партиции.
6. topic_all_request с одной партицией но replica-set = 2, постоянно пушим сообщения в этот топик, в процессе убиваем ноду с лидером партиции
7. consumer-1 должен выкидывать исключение в процессе чтения

### NORMAL

Для наглядности consumer должен записывать считанные сообщения в файл

1. Запустить нагрузочное тестирование и проверить скорость обработки сообщений в секунду
2. Разбить топик на партиции и проверить скорость обработки сообщений в секунду
3. Установить concurrency = 4 у consumer и снова замерить
4. Проверить как меняется производительность если установить параметр max.in.flight.requests.per.connection равным 1 и равным 5

### HARD

1. Решить проблему с упорядоченным чтением сообщений из разных партиций
2. Решить проблему с дублирующимся чтением сообщениц

## КОМАНДЫ

### KAFKA

1. `bin/kafka-topics --create --topic my_topic --partitions 3 --replication-factor 2` создание топика
2. `kafka-topics --list` получить список топиков
3. `bin/kafka-topics --describe --bootstrap-server localhost:9094 \  --topic my_topic`. Получить описание топика
4. `bin/kafka-topics.sh --alter --topic my_topic --config-file new_topic_config.properties` Обновляет параметры топика (кол-во партиций, репликацию и т.п.)
5. `/bin/kafka-topics.sh \ --bootstrap-server localhost:9092 \ --topic getting-started --delete` удаление топика целиком. (Асинхронная операция)
6. `/bin/kafka-consumer-groups.sh \ --bootstrap-server localhost:9092 --list` показать все consumer groups
7. `/bin/kafka-consumer-groups.sh \ --bootstrap-server localhost:9092 \ --group cli-consumer --describe --all-topics` Показать информацию о consumer groups


### ZOOKEEPER

1. `bin/zookeeper-shell.sh localhost:2181`. Подключение к zookeeper
2. `ls /brokers/topics`. Показать все топики
3. `bin/zookeeper-shell.sh localhost:2181 get /controller` позволяет узнать какая года Кафки в данный момент является контроллером (отвечает за управление кластером Кафки и прочие административные действия)
